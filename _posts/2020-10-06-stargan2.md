---
title: "[Review] (2020) StarGAN-v2"
categories:
  - GAN
tags:
  - Deep Learning
  - Style Learning
  - GAN
comments: true
toc: true
toc_sticky: true
toc_label: Title
toc_color: green
---


# 1. Introduction

- Domain : visual distictive한 카테고리 집합(gender)
- Style : 각 이미지가 갖는 고유한 외관 특성(hair, beard, shape)
- 기존의 StarGAN의 문제점은 label을 직접 가르켜주어야 했음(one-hot vector)
- 그래서, 주어진 source image에 대해 고정된 output만 나옴
- 해결하기 위해 다음과 같은 방법을 제안
    - Random gaussian noise로부터 Domain specific style code를 생매핑하는 mapping network
    - 주어진 reference image로부터 style code를 추출하는 style encoder
    - style code를 이용하여 이미지를 생성하는 Generator

# 2. Methods

![image](/assets/imgs/paper/2020-stargan2/00.png)

### 2.1. Generator

- Mapping network or Style encoder에 의해 제공되는 style code $s$와 input image $x$에 대해 domain-specific output $G(x,s)$을 생성함
- Adaptive Instance Normalization(AdaIN)을 적용하여 generator에 style code를 잘 적용할 수 있게 함
- image $y$를 제공하지 않아도 style code가 이를 설명가능함(diverse)

### 2.2. Mapping Network

- $s=F_y(z)$ for latent code $z$
- mapping network는 모든 이용가능한 domain을 표현할 수 있는 MLP로 이루어짐

### 2.3. Style Encoder

- $s=E_y(x)$ for latent code $x$
- reference image에 대한 style을 학습하도록 함

### 2.4. Discriminator

- Input image $x$가 domain $y$의 real image인지, $G(x,s)$의 fake image인지 판별함

### 2.5. Loss

- Adversarial loss
- Style reconstruction loss : $\mathcal{L}_{sty}=\mathbb{E}_{\mathrm{x},\tilde{y},\mathrm{z}} [|| \tilde{\mathrm{s}}- E_{\tilde{y}}(G(\mathrm{x},\tilde{s}))||_1 ]$
    - Input image를 특정 style로 변환 후, encoder로 style을 추출했을 때 차이
- Style diversification loss(diversity sensitive loss) : $\mathcal{L}_{ds}=\mathbb{E}_{\mathrm{x},\tilde{y},\mathrm{z}_1,\mathrm{z}_2} [|| G(\mathrm{x},\tilde{s}_1)- G(\mathrm{x},\tilde{s}_2)||_1 ]$
- Cycle consistency loss(preserving source characteristics) : $\mathcal{L}_{cyc}=\mathbb{E}_{\mathrm{x},y,\tilde{y},\mathrm{z}} [|| \mathrm{x}- G(G(\mathrm{x},\tilde{s}),\hat{s})||_1 ]$
    - 예를 들면, pose와 같은 domain-invariant한 특징에 대해 잘 보존되는지 여부를 판별
- Full objective : $\min_{G,F,E}\max_D  \mathcal{L}_{adv}+\lambda_{sty}\mathcal{L}_{sty}-\lambda_{ds}\mathcal{L}_{ds}+\lambda_{cyc}\mathcal{L}_{cyc}$

# 3. Experimental Results & Conclusion

![image](/assets/imgs/paper/2020-stargan2/01.png)

- FID(Frechet Inception Distance) : 실제 데이터의 분포와 생성된 데이터의 분포를 가우시안이라 가정하고, 두 분포의 wasserstein-2 distance를 계산(낮을 수록 좋음)
- LPIPS(Learned Perceptual Image Patch Similarity) : 인간의 시각 지각 판단과 유사한 판별 metric. 높을수록 좋음

![image](/assets/imgs/paper/2020-stargan2/02.png)

- StarGAN-v2는 makeup, hairstyle, beard 등 다양한 style에 대해 같은 input에 동일 이미지가 아닌 다양한 변화를 보여줌

![image](/assets/imgs/paper/2020-stargan2/03.png)

- Source에서 Reference 이미지의 style로 변화시킬 때도 baseline 모델들보다 월등히 좋은 output을 보여줌
- 기존 StarGAN의 단점을 잘 보완하고, style의 변화도 더 자연스러움
- 특히, multi-domain 간 변화가 잘 학습된다는 점에서 style encoding이 상당히 유의미함을 볼 수 있음

# References

- [https://github.com/clovaai/stargan-v2](https://github.com/clovaai/stargan-v2)
- [https://arxiv.org/abs/1912.01865](https://arxiv.org/abs/1912.01865)