---
title: "[Review] (2017) DSD: DENSE-SPARSE-DENSE Training For Deep Neural Networks"
categories:
  - Model Compression
tags:
  - Deep Learning
  - Model Compression
comments: true
toc: true
toc_sticky: true
toc_label: Title
toc_color: green
---

# 1. Introduction

- 일반적으로 큰(복잡한) 모델은 trainset의 내재된 pattern을 학습하기보다는 noise를 학습하여 overfitting이나 high variance를 가짐. 즉, 일반화가 잘 안됨.
- 반대로, 작은(단순한) 모델은 feature와 output의 상관관계가 적어져 underfitting과 high bias를 가짐.
- 저자가 제안하는 DSD 학습법은 Dense하게 선학습 후, pruning을 통해 모델을 sparse하게 제약을 두어 모델을 정규화하여 정말로 필요한 파라미터들만 학습한다. 마지막으로, 원래의 Dense한 파라미터들을 복원시켜 다시 학습하여 variance와 bias 모두를 잘 조절하는 모델을 만든다.

# 2. Methods

![image](/assets/imgs/paper/2017-dsd/00.png)

‌ 1) Dense step

- 일반적인 학습법과 동일하게 진행. 하지만, 우수한 성능 내기가 목적이 아닌 어느 뉴런이 정답을 도출하기 위해 잘 활성화되어있는지를 골라내기 위함임.

2) Sparse step

- Dense step에서 학습된 모델의 weight를 가지치기를 시도함.
- 각 layer의 N개의 weight값 중 sparsity에 따라 top-K번째에 있는 weight 값을 threshold로하여 그 이하의 값을 모두 masking으로 학습에서 제외시킴.
- 수렴할 때까지 학습 진행.

3) (Re)Dense step

- Sparse step에서 가지치기한 weight들을 0으로 초기화하여 복원시킴.
- Dropout과 같은 hyperparameter는 그대로 두고 Learning rate만 초기의 10분의 1로 세팅하여 재학습 진행.

![image](/assets/imgs/paper/2017-dsd/01.png)

# 3. Results & Conclusion‌

![image](/assets/imgs/paper/2017-dsd/02.png)

- 기존 Baseline model과 비교하여 DSD 학습법으로 진행 후 성능이 많이 좋아짐.

![image](/assets/imgs/paper/2017-dsd/03.png)

- Image Captioning과 Speech task에서도 baseline, sparse보다 유사하거나 좋은 성능을 보임

### 3.1. Escape Saddle Point

- 기존에도 saddle point를 극복하기 위해 다양한 optimization 방법들이 있음.
- DSD 학습법은 가지치기를 통해 network가 saddle point에 갖히지 않고 좀 더 나은 local minima나 global minimum에 수렴하도록 도와줌. (Simulated Annealing과 유사함)

### 3.2. Significantly Better Minima

- saddle point를 피하고나면 DSD 학습법은 더 좋은 minima를 찾는다는 내용.

### 3.3. Regularized and Sparse Training

- 가지치기 후 학습은 noise에 더 강하고, loss surface를 부드럽게하는 효과가 있음.
- 실제로 수많은 실험을 하여 sparse train 후, 평균적으로 variance가 낮아짐.

### 3.4. Robust re-initialization

- Weight 초기화는 deep learning에서 매우 중요한 부분.
- DSD는 처음뿐 아니라, sparse train후 한 번 더 초기화의 기회가 있어 더 좋은 solution 도출이 가능함.
- 마지막의 초기화를 0으로 하지 않고 다르게 진행하여도 좋은 시도임.

### 3.5. Break Symmetry

- 일반적으로 학습 후, hidden unit의 weight들은 대칭적인 모습을 보이는데 이는 표현이 그 부분으로 집중되어 결과에 좋지 않음.
- DSD는 가지치기를 통해 대칭성을 깨고 비대칭적인 weight 분포를 갖게하여 다양한 곳에 집중이 가능하게하여 성능을 올릴 수 있음.

- DSD는 가지치기를 잘 활용해 regularization을 수행한 훌륭한 학습 framework임.
- 가지치기를 하는 부분이나 다시 초기화하는 부분에 대해 다양한 방법을 시도해보면 더 좋은 결과가 나올 수 있지 않을까 함. (가지치기를 한 번만 하는 것이 아니라 학습 중간 중간 한다던가..)

# References

- [https://github.com/noirgif/DSD](https://github.com/noirgif/DSD)
- [https://arxiv.org/abs/1505.07818](https://arxiv.org/abs/1607.04381)
- [https://blog.lunit.io/2018/05/03/dsd-dense-sparse-dense-training-for-deep-neural-networks/](https://blog.lunit.io/2018/05/03/dsd-dense-sparse-dense-training-for-deep-neural-networks/)